#!/usr/bin/env python
# coding: utf-8

# In[ ]:


## IMPORTS 
import matplotlib.pyplot as plt # plotting library
import numpy as np # this module is useful to work with numerical arrays
import random 
import torch
import torchvision 
from torchvision import datasets
from torchvision.transforms import ToTensor
import torchvision.transforms as T

#import torchvision
#from torchvision import transforms
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import matplotlib.image as mpimg
import matplotlib.pyplot as plt 


#import splitfolders
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
#from tqdm.notebook import tqdm
import glob
import time
import PIL
from PIL import Image
import os
import cv2
import timm
from torchvision import models
from torchsummary import summary


# In[2]:


## BIG DATASET RESIZED 256 WITH DISTINCT SCENES FOR TRAIN AND TEST

# Original images 
#full_data_path='data/big_images_dataset_256'
train_images_path='raw/train/images'
test_images_path='raw/test/images'

#Associated depth maps
#full_depth_maps_path ='data/big_depth_maps_dataset_256'
train_depth_map_path='raw/train/depth_maps'
test_depth_map_path='raw/test/depth_maps'
DATA_LOADED=False


# In[3]:


# Activate cuda
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print(f'Selected device: {device}')


# In[62]:


## IMPORTING THE MIDAS NETWORK

#model_type = "DPT_Large"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)
#model_type = "DPT_Hybrid"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)
model_type = "MiDaS_small"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)

midas = torch.hub.load("intel-isl/MiDaS", model_type, trust_repo=True)

# LOADING THE NECESSARY MIDAS TRANSFORM


midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms", trust_repo=True)
transform_midas = midas_transforms.small_transform


# In[16]:


#Important= travailler avec les images brutes.
def load_data(data_path, file_type, new_shape=(256, 128)):
    images=[]
    t0=time.time()
    if not DATA_LOADED:
      print('Loading data...')
      list_dir = os.listdir(data_path)
      list_dir=sorted(list_dir, key=lambda x: x.lower()) 
      for filename in list_dir:
            if file_type=='image':
                img = np.array(Image.open(os.path.join(data_path, filename)))
                if img is not None:
                    if transform:
                        img = transform_midas(img)
                    images.append(img)
                
            if file_type=='depth_map':
                img = np.array(Image.open(os.path.join(data_path, filename)))
                #img=np.array(Image.open(filename))
                if img is not None:
                    img=cv2.resize(img, new_shape, interpolation=cv2.INTER_NEAREST)
                    images.append(img)
            
            
      print(f'Loaded data in: {time.time()-t0 } s')
    return images




#transform=True
#train_images=load_data(train_images_path, 'image')
#test_images=load_data(test_images_path, 'image')
#train_depth_maps=load_data(train_depth_map_path, 'depth_map')
#test_depth_maps=load_data(test_depth_map_path, 'depth_map')

transform=False
raw_train_images=load_data(train_images_path, 'image')
raw_test_images=load_data(test_images_path, 'image')

#test=load_data(train_images_path, 'image')
#test1=load_data(train_depth_map_path, 'depth_map')


# In[11]:


plt.imshow(np.transpose(train_images[0].squeeze(0), [1,2,0]))


# In[19]:


plt.imshow(raw_train_images[43])


# In[61]:


#Check that depth maps and images are well paired 
new_shape=(256,128)
ind=np.random.randint(680)
figs, axs= plt.subplots(3,1, figsize=(20,20))
axs[0].imshow(cv2.resize(raw_test_images[ind], new_shape, interpolation=cv2.INTER_NEAREST))
axs[0].imshow(test_depth_maps[ind], alpha=0.5)
axs[1].imshow(cv2.resize(raw_test_images[ind], new_shape, interpolation=cv2.INTER_NEAREST))
axs[2].imshow(test_depth_maps[ind])
plt.show()


# In[21]:


ind=np.random.randint(680)

print(np.shape(train_depth_maps[ind]))
print(np.shape(train_images[ind]))


print('\n Resized depth map, in millimeters \n ')
print(train_depth_maps[ind])


print('\n Resized depth map, in meters \n')
print(train_depth_maps[ind]/1000)

#Inverse depth map, in meters 
inv_depth_map=1/(train_depth_maps[ind]/1000)
inv_depth_map[inv_depth_map==np.inf]=0

#inv_depth_map=1/(train_depth_maps[ind])

#Comparison

figs, axs= plt.subplots(1,4, figsize=(20,30))
axs[0].imshow(train_depth_maps[ind], cmap='gray')
axs[3].imshow(np.transpose(train_images[ind].squeeze(), [1,2,0]))
axs[2].imshow(cv2.resize(raw_train_images[ind], new_shape, interpolation=cv2.INTER_NEAREST))
axs[1].imshow(inv_depth_map, cmap='gray')
plt.show()


# In[22]:


class Original_and_Depth_Map(torch.utils.data.Dataset):

    def __init__(self, listA, listB, transformA=None, transformB=None):
        self.listA= listA
        self.listB= listB
        self.datasetA = []
        self.datasetB = []
       
    #Making sure all images have the same size before creating an array
        for el, img in zip(self.listA, self.listB):
            self.datasetA.append(el.squeeze())
            self.datasetB.append(img)
            
        self.transformA= transformA
        self.transformB= transformB
        
    def __getitem__(self, index):
        xA = self.datasetA[index]
        xB = self.datasetB[index]
        
        if self.transformA:
            xA=self.transformA(xA)
        if self.transformB:
            xB=self.transformB(xB)
        
    
        return np.array(xA), np.array(xB)
            

    def __len__(self):
        return len(self.datasetA)


# In[23]:


batch_size = 8

# CREATING THE TRAIN VAL AND TEST DATASETS 

## SCENE DISTINCTION PROCESS 



train_and_val_dataset =Original_and_Depth_Map(train_images, train_depth_maps)
test_dataset =Original_and_Depth_Map(test_images, test_depth_maps)

train_size= 2500
val_size = 345
train_dataset, val_dataset= torch.utils.data.random_split(train_and_val_dataset, [train_size, val_size])



train_loader = DataLoader(train_dataset, batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size, shuffle=True)
# Is it a good idea to to a test loader? 


# In[ ]:


for el in train_images:
    print(np.shape(el))


# In[24]:


#LENGHT TEST 

print(len(train_dataset), len(test_dataset), len(val_dataset))

image, depth= next(iter(test_loader))
#image, depth= next(iter(train_loader))
midas.to(device)
midas.train()
image.to(device)

print('batch depth size', depth.size())
print('batch image size', image.size())


print(type(image))
image=image.squeeze()

print(image.size())
#image = np.transpose(image, [0,3,1,2])
image=image.type(torch.cuda.FloatTensor)
output=midas(image)
test_output=output[0]
image[0]=image[0].cpu()
#test_image=image[0]*(torch.max(image[0])-torch.min(image[0]))+torch.max(image[0])
plt.imshow(np.transpose(image[0].cpu(), [1,2,0]))
plt.figure()
plt.imshow(test_output.cpu().detach().numpy(), cmap='gray')

print('**** ORIGINAL INVERSE DEPTH MAP*****')
print(depth[0])

print('***** ORIGINAL DEPTH MAP****')
print(1/(depth[0]))

print('***** RECONSTRUCTED INVERSE DEPTH MAP****')
print(test_output)


# In[25]:


# MIDAS PRE IMPLEMENTED LOSSES


def compute_scale_and_shift(prediction, target, mask):
    # system matrix: A = [[a_00, a_01], [a_10, a_11]]
    a_00 = torch.sum(mask * prediction * prediction, (1, 2))
    a_01 = torch.sum(mask * prediction, (1, 2))
    a_11 = torch.sum(mask, (1, 2))

    # right hand side: b = [b_0, b_1]
    b_0 = torch.sum(mask * prediction * target, (1, 2))
    b_1 = torch.sum(mask * target, (1, 2))

    # solution: x = A^-1 . b = [[a_11, -a_01], [-a_10, a_00]] / (a_00 * a_11 - a_01 * a_10) . b
    x_0 = torch.zeros_like(b_0)
    x_1 = torch.zeros_like(b_1)

    det = a_00 * a_11 - a_01 * a_01
    valid = det.nonzero()

    x_0[valid] = (a_11[valid] * b_0[valid] - a_01[valid] * b_1[valid]) / det[valid]
    x_1[valid] = (-a_01[valid] * b_0[valid] + a_00[valid] * b_1[valid]) / det[valid]

    return x_0, x_1


def reduction_batch_based(image_loss, M):
    # average of all valid pixels of the batch

    # avoid division by 0 (if sum(M) = sum(sum(mask)) = 0: sum(image_loss) = 0)
    divisor = torch.sum(M)

    if divisor == 0:
        return 0
    else:
        return torch.sum(image_loss) / divisor


def reduction_image_based(image_loss, M):
    # mean of average of valid pixels of an image

    # avoid division by 0 (if M = sum(mask) = 0: image_loss = 0)
    valid = M.nonzero()

    image_loss[valid] = image_loss[valid] / M[valid]

    return torch.mean(image_loss)


def mse_loss(prediction, target, mask, reduction=reduction_batch_based):

    M = torch.sum(mask, (1, 2))
    res = prediction - target
    image_loss = torch.sum(mask * res * res, (1, 2))

    return reduction(image_loss, 2 * M)



def gradient_loss(prediction, target, mask, reduction=reduction_batch_based):

    M = torch.sum(mask, (1, 2))

    diff = prediction - target
    diff = torch.mul(mask, diff)

    grad_x = torch.abs(diff[:, :, 1:] - diff[:, :, :-1])
    mask_x = torch.mul(mask[:, :, 1:], mask[:, :, :-1])
    grad_x = torch.mul(mask_x, grad_x)

    grad_y = torch.abs(diff[:, 1:, :] - diff[:, :-1, :])
    mask_y = torch.mul(mask[:, 1:, :], mask[:, :-1, :])
    grad_y = torch.mul(mask_y, grad_y)

    image_loss = torch.sum(grad_x, (1, 2)) + torch.sum(grad_y, (1, 2))

    return reduction(image_loss, M)


class MSELoss(nn.Module):
    def __init__(self, reduction='batch-based'):
        super().__init__()

        if reduction == 'batch-based':
            self.__reduction = reduction_batch_based
        else:
            self.__reduction = reduction_image_based

    def forward(self, prediction, target, mask):
        return mse_loss(prediction, target, mask, reduction=self.__reduction)


class GradientLoss(nn.Module):
    def __init__(self, scales=4, reduction='batch-based'):
        super().__init__()

        if reduction == 'batch-based':
            self.__reduction = reduction_batch_based
        else:
            self.__reduction = reduction_image_based

        self.__scales = scales

    def forward(self, prediction, target, mask):
        total = 0

        for scale in range(self.__scales):
            step = pow(2, scale)

            total += gradient_loss(prediction[:, ::step, ::step], target[:, ::step, ::step],
                                   mask[:, ::step, ::step], reduction=self.__reduction)

        return total


class ScaleAndShiftInvariantLoss(nn.Module):
    def __init__(self, alpha=0.5, scales=4, reduction='batch-based'):
        super().__init__()

        self.__data_loss = MSELoss(reduction=reduction)
        self.__regularization_loss = GradientLoss(scales=scales, reduction=reduction)
        self.__alpha = alpha

        self.__prediction_ssi = None

    def forward(self, prediction, target, mask):

        scale, shift = compute_scale_and_shift(prediction, target, mask)
        self.__prediction_ssi = scale.view(-1, 1, 1) * prediction + shift.view(-1, 1, 1)

        total = self.__data_loss(self.__prediction_ssi, target, mask)
        if self.__alpha > 0:
            total += self.__alpha * self.__regularization_loss(self.__prediction_ssi, target, mask)

        return total

    def __get_prediction_ssi(self):
        return self.__prediction_ssi

    prediction_ssi = property(__get_prediction_ssi)


# In[26]:


# METRICS COMPUATION

def safe_log10(x, eps=1e-10):     
    result = np.where(x > eps, x, -10)     
    np.log10(result, out=result, where=result > 0)     
    return result


def compute_errors(gt, pred):
    """Computation of error metrics between predicted and ground truth depths
       Metrics have to be computed independently on each image 
    """
    abs_rel_batch, sq_rel_batch, rmse_batch, rmse_log_batch, a1_batch, a2_batch, a3_batch= [], [], [], [], [], [], []
    
    for gt_im, pred_im in zip(gt, pred):
        eps=1
        thresh = np.maximum((gt_im / (pred_im+eps)), (pred_im / (gt_im+eps)))
        thresh.type(torch.FloatTensor)

        a1 = (1*(thresh < 1.25     )).numpy()
        a2 = (1*(thresh < 1.25 ** 2)).numpy()
        a3 = (1*(thresh < 1.25 ** 3)).numpy()

        a1=a1.mean()
        a2=a2.mean()
        a3=a3.mean()
        

        rmse= (gt - pred) ** 2
        rmse= np.sqrt(rmse.mean())

        rmse_log= (safe_log10(gt) - safe_log10(pred) )** 2
        rmse_log = np.sqrt(rmse_log.mean())
        
        #The input and ouput musst be in the same range so that those metrics have a sense
        gt_max, gt_min= torch.max(gt), torch.min(gt)
        pred_max, pred_min=torch.max(gt), torch.min(pred)
        
        gt_r=(gt-gt_max)/(eps+gt_max-gt_min)
        pred_r=(pred-pred_max)/(eps+pred_max-pred_min)
        
        
        
        abs_rel=(np.abs(gt_r-pred_r)/(eps+gt_r)).mean()
        sq_rel=((gt_r-pred_r)**2/(eps+gt_r)).mean()
        
        abs_rel_batch.append(abs_rel)
        sq_rel_batch.append(sq_rel)
        rmse_batch.append(rmse.item())
        rmse_log_batch.append(rmse_log.item())
        a1_batch.append(a1.item())
        a2_batch.append(a2.item())
        a3_batch.append(a3.item())

    return abs_rel_batch, sq_rel_batch, rmse_batch, rmse_log_batch, a1_batch, a2_batch, a3_batch


# In[27]:


#Training function

def train_epoch(network, device, dataloader, optimizer, loss_function, print_every = 1000):
    """ Trains the simple model for one epoch. losses_resolution indicates how often training_loss should be printed and stored. """
    network.train()
    network.to(device)
    
    train_losses = []
    

    # Iterate the dataloader (We do not need the label value which is 0 here, the depth maps are the labels)
    iter = 0
    for image, depth_map in dataloader:   
      #Moving to GPU
      image.to(device)
      d = depth_map.to(device)
      
      #Right size and type
      image=image.squeeze()
      image = image.type(torch.cuda.FloatTensor)
      d=d.type(torch.cuda.FloatTensor)


      #Going through the network
      inv_d_hat = network(image)
      
      #Computing the loss, storing it
      mask=1.0*(d!=0)
      
      #Not taking into account the zones where there is no data available (mask)
        
      #From depth map in mm to m and then to the inverse depth map
      d/=1000
      inv_d=1/d
      inv_d[inv_d==np.inf]=0
      #loss=loss_function(inv_d_hat.squeeze()[mask==1],  inv_d[mask==1]).float()
      loss=loss_function(inv_d_hat.squeeze(),  inv_d, mask).float()


      # Backward pass
      optimizer.zero_grad()  # Sets the gradients attached to the parameters objects to zero.
      loss.backward()  # Uses the gradient object attached to the loss to recursively compute the gradients of the parameters of the network (and store their value in the gradient objects attached to said parameters)
      optimizer.step()  # Actually chages the values of the parameters using their gradients, computed on the previous line of code.
      
      #Store batch loss
      batch_loss = loss.item()/d.shape[0]
      train_losses.append(batch_loss)
    
      #Display
      #if iter%print_every == 0:
          #print(f'\t partial train loss (single batch): {batch_loss:.2f}')
    return train_losses

"""midas = torch.hub.load("intel-isl/MiDaS", model_type, trust_repo=True)
lr=1e-4
torch.cuda.empty_cache()
network=midas
weight_decay=1e-4
#loss_function=nn.L1Loss()
loss_function=ScaleAndShiftInvariantLoss()
dataloader=train_loader
optimizer=torch.optim.Adam(midas.parameters(), lr=lr, weight_decay=weight_decay)
for i in range(20):
    train_losses=train_epoch(network, device, dataloader, optimizer, loss_function)
    print(train_losses[-1])
    """


# In[28]:


## Testing function

def validation_epoch(network, device, loss_function, dataloader):
    "Set evaluation mode for encoder and decoder"
    network.to(device)
    network.eval()  # evaluation mode, equivalent to "network.train(False)""
    val_loss = 0
    
    rmse_epoch=np.array([])
    rmse_log_epoch=np.array([])
    abs_rel_epoch=np.array([])
    sq_rel_epoch=np.array([])
    a1_epoch=np.array([])
    a2_epoch=np.array([])
    a3_epoch=np.array([])
    
    
    with torch.no_grad(): # No need to track the gradients
        for image, depth_map in dataloader:
            #Moving to GPU
            
            image.to(device)
            d = depth_map.to(device)
            #Applying the necessary transforms
            #image=image.squeeze()
            image = image.type(torch.cuda.FloatTensor)
            d=d.type(torch.cuda.FloatTensor)
            
            #Going through the network
            inv_d_hat = network(image)
            
            mask=1.0*(d!=0)
            d/=1000
            inv_d=1/d
            inv_d[inv_d==np.inf]=0
            


            
          

            #Computing the loss, storing it
            loss=loss_function(inv_d_hat.squeeze(),  inv_d, mask).float()
            
            val_loss += loss.item()/d.shape[0]
            
            #Store values for metrics 
            abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3 = compute_errors(inv_d.detach().cpu(), inv_d_hat.detach().cpu())

            abs_rel_epoch=np.concatenate((abs_rel, abs_rel_epoch))
            sq_rel_epoch=np.concatenate((sq_rel, sq_rel_epoch))
            rmse_epoch=np.concatenate((rmse, rmse_epoch))
            rmse_log_epoch=np.concatenate((rmse_log, rmse_log_epoch))
            a1_epoch=np.concatenate((a1, a1_epoch))
            a2_epoch=np.concatenate((a2, a2_epoch))
            a3_epoch=np.concatenate((a3, a3_epoch))

    return val_loss/len(dataloader), abs_rel_epoch, sq_rel_epoch, rmse_epoch, rmse_log_epoch, a1_epoch, a2_epoch, a3_epoch


# In[29]:


#Test validation loss
loss_function=ScaleAndShiftInvariantLoss()
val_loss, abs_rel_epoch, sq_rel_epoch, rmse_epoch, rmse_log_epoch, a1_epoch, a2_epoch, a3_epoch= validation_epoch(midas, device, loss_function, val_loader)


# In[36]:


## Plotting function

def plot__outputs(network, number_outputs=5, random_plots=True, indices=None, title=None):
    """
    When random_plots = True, different images are taken between each epoch.
    When random_plots = False, indices indicates what images to plot
    """
    fig = plt.figure(figsize=(5*number_outputs,8.5))
    if title is not None:
        fig.suptitle(f'Results at the end of epoch {title}',  fontsize='large', fontweight='bold')
  
  #Selection of random images to plot within the dataset or a select set of indices
    if random_plots:
        plotted_samples = np.random.choice(len(test_dataset), number_outputs)
    else:
        plotted_samples = indices
  
    for i in range(number_outputs):
        ax = plt.subplot(3, number_outputs, i+1)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

       # plt.subplot(nb_rows, nb_cols, num)  where num is indexed from 1
        raw_image=raw_test_images[plotted_samples[i]]
        image = test_dataset[plotted_samples[i]][0]


        #plt.imshow(np.transpose(raw_image, [1,2,0]))
        plt.imshow(raw_image)
        network.to(device)
        network.eval()
        
        with torch.no_grad():
            t=T.ToTensor()
            #print(t(image).size())
            output_img=network(np.transpose(t(image), [1,2,0]).type(torch.cuda.FloatTensor).unsqueeze(0))
            output_img=output_img.cpu().squeeze().numpy()
            #output_img=1/output_img
            #output_img[output_img==np.inf]=0
            #print(output_img)
      
            
        if i == number_outputs//2:
            ax.set_title('Original images')
    
      

        ax = plt.subplot(3, number_outputs, i + 1 + number_outputs)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        plt.imshow(output_img, cmap='gray')

    
        if i == number_outputs//2:
            ax.set_title('Reconstructed inverse depth maps')
        ax=plt.subplot(3, number_outputs, i + 1 + 2*number_outputs)
        
        
        
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        
        inv_depth_map=1/test_dataset[plotted_samples[i]][1]
        
        inv_depth_map[inv_depth_map==np.inf]=0
        plt.imshow(inv_depth_map, cmap='gray')
        
        if i == number_outputs//2:
            ax.set_title('Original inverse depth maps')

plot__outputs(midas)


# In[ ]:


## Training and Validation processes


## Choice of different parameters
lr=1e-4
weight_decay=1e-4 #Weight regularization to avoid overfitting
number_of_plotted_images = 5
plot_images_every_n_epochs = 2
random_plots = False
indices_to_plot = np.arange(number_of_plotted_images)
num_epochs= 200
optim=torch.optim.Adam(midas.parameters(), lr=lr, weight_decay=weight_decay, eps=1e-20)
lowest_RMSE_value=1e3


## Choice of the loss
mse=nn.MSELoss()
l1=nn.L1Loss()
loss_function=ScaleAndShiftInvariantLoss()

train_losses = np.array([])
val_loss, abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3=validation_epoch(midas, device, loss_function, val_loader)
val_losses = np.array([val_loss]) #first evaluation before training



rmse=np.array(rmse)
rmse_log=np.array(rmse_log)
abs_rel=np.array(abs_rel)
sq_rel=np.array(sq_rel)
a1=np.array(a1)
a2=np.array(a2)
a3=np.array(a3)


## Training process

print_metrics=True

train_net=True

if train_net:
    for epoch in range(num_epochs):
        #if epoch>50:
            #if epoch%10==0:
                #lr/=5
                #optim=torch.optim.Adam(midas.parameters(), lr=lr, weight_decay=weight_decay, eps=1e-20)
       
        #Optimising the model 
        train_losses_epoch= train_epoch(midas, device, train_loader,optim, loss_function, print_every=100)
       
        #Assessing the model performance during training
        val_loss_epoch, abs_rel_epoch, sq_rel_epoch, rmse_epoch, rmse_log_epoch, a1_epoch, a2_epoch, a3_epoch  = validation_epoch(midas, device, loss_function, val_loader)

        #Storing 
        train_losses = np.concatenate((train_losses, train_losses_epoch))
        val_losses= np.append(val_losses, val_loss_epoch)


        abs_rel=np.concatenate((abs_rel_epoch, abs_rel))
        sq_rel=np.concatenate((sq_rel_epoch, sq_rel))
        rmse=np.concatenate((rmse_epoch, rmse))
        rmse_log=np.concatenate((rmse_log_epoch, rmse_log))
        a1=np.concatenate((a1_epoch, a1))
        a2=np.concatenate((a2_epoch, a2))
        a3=np.concatenate((a3_epoch, a3))


        print(f'\n EPOCH {epoch + 1}/{num_epochs} \t train loss {train_losses_epoch[-1]:.4f} \t val loss {val_loss_epoch:.4f} ')
        if print_metrics:
            print(f'\n RMSE {rmse.mean():.4f} \t RMSLE {rmse_log.mean():.4f} \t abs rel {abs_rel.mean():.4f} \t sq rel {sq_rel.mean():.4f}')
            
        # Storing the network weights if the RMSE decreazes
        if rmse.mean()<lowest_RMSE_value:
            lowest_RMSE_value=rmse.mean()
            torch.save(midas.state_dict(), 'save_training/midas_200_epochs.pt')

       #Plotting reconstruction results at the end of each epoch to see the evolution (on the same outputs)
        if epoch%10==0:
            plot__outputs(midas, number_outputs=number_of_plotted_images, random_plots=random_plots, indices=indices_to_plot, title=epoch)
        
    


# In[ ]:


train_losses_mean=[]
nb_batches=len(train_dataset)//batch_size
for i in range(0, len(train_losses), nb_batches):
    
    losses_batches=train_losses[i:(i+1)*nb_batches]
    print(f'step {i}')
    #print(train_losses[i:(i+1)*nb_batches])
    print(losses_batches.mean())
    train_losses_mean.append(losses_batches.mean())
    
       
   
        
print(len(train_losses_mean))
train_losses_mean=np.array(train_losses_mean)

    


# In[ ]:


##### Plotting reconstructions after 150 epochs

plot__outputs(midas, number_outputs=number_of_plotted_images, random_plots=True, indices=indices_to_plot, title=epoch)
print(train_losses[-1], val_losses[-1], abs_rel[-1], sq_rel[-1], rmse[-1], rmse_log[-1], a1[-1], a2[-1], a3[-1])
# The reconstructed inverse depth map give even more details than the ground truth inverse depth maps (especially details in the background of the images)


# In[40]:


def plot_learning_curves(train_losses, val_losses):
    """ Plots the learning curve. 
    Losses resolution (how many times the loss is displayed) is one value per batch for the training losses, 
    and one value per epoch for the validation. """
    
    #iterations_train = np.arange(0, batch_size*train_losses.size, batch_size)
    iterations_train = np.arange(0, len(train_dataset)*train_losses.size, len(train_dataset))
    iterations_validation = np.arange(0, len(train_dataset)*val_losses.size, len(train_dataset))

    fig, ax = plt.subplots(figsize=(25, 10))
    #What happens during training 
    ax.plot(iterations_train, train_losses, color="blue", label="training loss")

    #What happens during validation
    ax.plot(iterations_validation, val_losses, color="orange", label = "validation loss")
    
    
    ax.set(xlabel="iterations", ylabel="loss", title="Training and validation losses over iterations")
    ax.grid()
    ax.legend()

#plot_learning_curves(train_losses, val_losses)

plot_learning_curves(train_losses_mean, val_losses)


# In[ ]:


def plot_metrics(abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3):
    """ Plots the evolution of the different metrics. Displays one value per epoch"""
    
    #iterations = np.arange(0, len(abs_rel))
    iterations_train = np.arange(0, len(train_dataset)*train_losses.size, len(train_dataset))
    
    
    fig, ax = plt.subplots(5,1, figsize=(20,30))
    ax[0].plot(iterations, abs_rel, color="red", label="Absolute relative error")
    ax[1].plot(iterations, sq_rel, color="green", label = "Square relative error")
    ax[2].plot(iterations, rmse, color="olive", label = "RMSE")
    ax[3].plot(iterations, rmse_log, color="cyan", label = "RMSLE")
    ax[4].plot(iterations, a1, color="brown", label = r"$\delta \leq 1.25$")
    ax[4].plot(iterations, a2, color="gray", label = r"$\delta \leq 1.25^2$")
    ax[4].plot(iterations, a3, color="gold", label = r"$\delta \leq 1.25^3$")
    
    for i in range(5):
        ax[i].set(xlabel="iterations")
        ax[i].legend()

plot_metrics(abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3)

