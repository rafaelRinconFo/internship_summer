{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_colmap as c\n",
    "import numpy as np\n",
    "import torch \n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "#ghost_city_bin_path='raw/test/bin_files/ghost_city_bins/images.bin'\n",
    "#medium_structure_bin_path='raw/test/bin_files/medium_structure_bins/images.bin'\n",
    "\n",
    "ghost_city_bin_path='/home/server/Ines/models/raw/test/bin_files/ghost_city_bins/images.bin'\n",
    "medium_structure_bin_path='/home/server/Ines/models/raw/test/bin_files/medium_structure_bins/images.bin'\n",
    "thermitiere_bin_path='/home/server/Ines/models/raw/train/bin_files/thermitiere_bins/images.bin'\n",
    "old_cliff_bin_path='/home/server/Ines/models/raw/train/bin_files/old_rainbow_cliff_bins/images.bin'\n",
    "\n",
    "test_bin_paths=[ghost_city_bin_path, medium_structure_bin_path]\n",
    "train_bin_paths=[old_cliff_bin_path, thermitiere_bin_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the stereo datasets, dataloaders, containing Colmap information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_objects=c.read_images_binary(medium_structure_bin_path)\n",
    "ind_map_ms=list(ms_objects.keys())\n",
    "gc_objects=c.read_images_binary(ghost_city_bin_path)\n",
    "ind_map_gc=list(gc_objects.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[337, 336, 335, 334, 333, 332, 331, 330, 329, 328, 327, 326, 325, 324, 323, 322, 321, 320, 319, 318, 317, 316, 315, 314, 313, 312, 311, 310, 309, 308, 307, 306, 305, 304, 303, 302, 301, 300, 299, 298, 297, 296, 295, 294, 293, 292, 291, 290, 289, 288, 287, 286, 285, 284, 283, 282, 281, 280, 279, 278, 277, 276, 275, 274, 273, 272, 271, 270, 269, 268, 267, 266, 265, 264, 263, 262, 261, 260, 259, 258, 257, 256, 255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 16, 15, 351, 14, 181, 350, 13, 180, 349, 12, 179, 348, 11, 178, 347, 10, 177, 346, 9, 176, 345, 8, 175, 344, 7, 174, 339, 2, 169, 338, 1, 168, 340, 3, 170, 341, 4, 171, 342, 5, 172, 425, 88, 426, 89, 427, 90, 428, 91, 429, 92, 430, 93, 431, 94, 432, 95, 433, 96, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 112, 450, 113, 451, 114, 452, 115, 453, 116, 454, 117, 455, 118, 456, 119, 457, 120, 458, 121, 459, 122, 460, 123, 461, 124, 462, 125, 463, 126, 464, 127, 465, 128, 466, 129, 467, 130, 468, 131, 469, 132, 470, 133, 471, 134, 472, 135, 473, 136, 474, 137, 475, 138, 476, 139, 477, 140, 478, 141, 479, 142, 480, 143, 481, 144, 482, 145, 483, 146, 484, 147, 485, 148, 486, 149, 487, 150, 488, 151, 489, 152, 490, 153, 491, 154, 492, 155, 493, 156, 494, 157, 495, 158, 496, 159, 497, 160, 498, 161, 499, 162, 500, 163, 501, 164, 502, 165, 503, 166, 504, 167, 505, 506, 507, 508, 509, 510, 173, 511, 512, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 585, 586, 587, 588, 590, 589, 584, 583, 582, 581, 580, 579, 578, 577, 576, 575, 554, 553, 552, 551, 550, 549, 548, 547, 546, 545, 544, 543, 542, 541, 540, 539, 538, 537, 536, 535, 534, 533, 532, 531, 530, 529, 528, 527, 526, 525, 524, 523, 522, 521, 520, 519, 518, 517, 516, 515, 514, 513, 424, 87, 423, 86, 422, 85, 421, 84, 420, 83, 419, 82, 418, 81, 417, 80, 416, 79, 415, 414, 413, 412, 411, 410, 409, 408, 407, 406, 405, 404, 403, 402, 401, 400, 399, 398, 397, 396, 395, 394, 393, 392, 391, 390, 389, 388, 387, 386, 385, 384, 383, 382, 381, 380, 379, 378, 377, 376, 375, 374, 373, 36, 372, 35, 371, 34, 370, 33, 369, 32, 368, 31, 367, 30, 366, 29, 365, 28, 364, 27, 363, 26, 362, 25, 361, 24, 360, 23, 359, 22, 358, 21, 357, 20, 356, 19, 355, 18, 354, 17, 353, 352, 6, 343, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 33, 34, 35, 36, 79, 80, 1, 81, 2, 82, 3, 83, 4, 84, 5, 85, 6, 86, 7, 87, 8, 88, 9, 89, 10, 90, 11, 101, 22, 102, 23, 29, 103, 24, 30, 104, 25, 32, 106, 27, 31, 105, 26, 100, 21, 99, 20, 98, 19, 97, 18, 96, 17, 95, 16, 94, 15, 93, 14, 92, 13, 91, 12, 28, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n"
     ]
    }
   ],
   "source": [
    "total_ind_map=ind_map_gc+ind_map_ms\n",
    "#print(total_ind_map)\n",
    "#total_ind_map.sort()\n",
    "print(total_ind_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionnaries for the train set \n",
    "\n",
    "th_objects=c.read_images_binary(thermitiere_bin_path)\n",
    "or_objects=c.read_images_binary(old_cliff_bin_path)\n",
    "th_ind_map=list(th_objects.keys())\n",
    "or_ind_map=list(or_objects.keys())\n",
    "train_ind_map=th_ind_map+or_ind_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_and_ind_maps(path1, path2):\n",
    "    objects_1=c.read_images_binary(path1)\n",
    "    objects_2=c.read_images_binary(path2)\n",
    "    ind_map_1=list(objects_1.keys())\n",
    "    ind_map_2=list(objects_2.keys())\n",
    "    total_ind_map=ind_map_1+ind_map_2\n",
    "    return objects_1, objects_2, total_ind_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire process\n",
    "\n",
    "# extraction before the dataloader!!\n",
    "gc_objects, ms_objects, test_ind_map=dict_and_ind_maps(ghost_city_bin_path, medium_structure_bin_path)\n",
    "or_objects, th_objects, train_ind_map=dict_and_ind_maps(old_cliff_bin_path, thermitiere_bin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_and_Pose_Pairs(torch.utils.data.Dataset):\n",
    "\n",
    "    def create_dict(self, path_bin):\n",
    "        \"\"\"Input: List of strings\n",
    "           Output: List of dictionnaries containing Colmap information \"\"\"\n",
    "\n",
    "        total_dict=[]\n",
    "        for path in path_bin:\n",
    "            dict=c.read_images_binary(path)\n",
    "            total_dict.append(dict)\n",
    "        return total_dict\n",
    "\n",
    "    def get_seq_ids(self, list_dict):\n",
    "        \"\"\" Input : List of dictionnaries \n",
    "            Output: List, with len=total number of images\n",
    "            Maps ind_dataset <--> image sequence to which the image belongs\"\"\"\n",
    "        seq_ids=[]\n",
    "        for i in range(len(list_dict)):\n",
    "            seq_ids+=list(i*np.ones(len(list_dict[i])))\n",
    "        return seq_ids\n",
    "\n",
    "    def get_ind_map(self, list_dict):\n",
    "        \"\"\" Maps ind_dataset <--> index within the image sequence\"\"\"\n",
    "        ind_map=[]\n",
    "        for dict in list_dict:\n",
    "            ind_map+=list(dict.keys())\n",
    "        return ind_map\n",
    "\n",
    "    def __init__(self, images_paths, bin_paths, network_name, transform_images=None, new_shape=(256,128)):\n",
    "        \n",
    "        self.images_path=images_paths[0]\n",
    "        self.bin_paths=bin_paths\n",
    "        self.transform_images= transform_images\n",
    "        self.network_name= network_name\n",
    "        self.new_shape=new_shape\n",
    "        \n",
    "\n",
    "        ## Processing the folder containing the jpg images \n",
    "        self.list_dir_images = os.listdir(self.images_path)\n",
    "        self.list_dir_images=sorted(self.list_dir_images, key=lambda x: x.lower()) \n",
    "\n",
    "        ## Creating the dataset dictionnary\n",
    "        self.total_dict=self.create_dict(self.bin_paths)\n",
    "\n",
    "        # Knowing from which sequence of images each sample comes from:\n",
    "        self.seq_ids=self.get_seq_ids(self.total_dict)\n",
    "\n",
    "        # Getting the ind_map : ind_in_dataset <--> ind_in_image_sequence\n",
    "        self.ind_map=self.get_ind_map(self.total_dict)\n",
    "   \n",
    "    \n",
    "    def get_T_matrix(self, seq_id, img_id, dict):\n",
    "        # Getting the information from Colmap pose calculation\n",
    "        tvec=dict[seq_id][img_id].tvec\n",
    "        qvec=dict[seq_id][img_id].qvec\n",
    "\n",
    "        #Size check\n",
    "        if len(qvec)!=4:\n",
    "            return 'SizeError: A quaternion contains 4 values'\n",
    "\n",
    "        if len(tvec)!=3:\n",
    "            return 'SizeError: A translation contains 3 values'\n",
    "        \n",
    "        # Extract the values from Q\n",
    "        q0 = qvec[0]\n",
    "        q1 = qvec[1]\n",
    "        q2 = qvec[2]\n",
    "        q3 = qvec[3]\n",
    "        \n",
    "        # First row of the rotation matrix\n",
    "        r00 = 2 * (q0 * q0 + q1 * q1) - 1\n",
    "        r01 = 2 * (q1 * q2 - q0 * q3)\n",
    "        r02 = 2 * (q1 * q3 + q0 * q2)\n",
    "        \n",
    "        # Second row of the rotation matrix\n",
    "        r10 = 2 * (q1 * q2 + q0 * q3)\n",
    "        r11 = 2 * (q0 * q0 + q2 * q2) - 1\n",
    "        r12 = 2 * (q2 * q3 - q0 * q1)\n",
    "        \n",
    "        # Third row of the rotation matrix\n",
    "        r20 = 2 * (q1 * q3 - q0 * q2)\n",
    "        r21 = 2 * (q2 * q3 + q0 * q1)\n",
    "        r22 = 2 * (q0 * q0 + q3 * q3) - 1\n",
    "        \n",
    "        # 3x3 rotation matrix\n",
    "        rot_matrix = np.array([[r00, r01, r02],\n",
    "                            [r10, r11, r12],\n",
    "                            [r20, r21, r22]])\n",
    "\n",
    "        # Full T_world-->cam matrix\n",
    "        T=np.array([[r00, r01, r02, tvec[0]],\n",
    "                    [r10, r11, r12, tvec[1]],\n",
    "                    [r20, r21, r22, tvec[2]],\n",
    "                    ])\n",
    "        \n",
    "        # Converting the array to a tensor (batch processing later)\n",
    "        return torch.from_numpy(T)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" \n",
    "        Input: index: an integer giving the position in the dataset\n",
    "\n",
    "        Output: Dictionnary\n",
    "\n",
    "        Keys in the dictionnary are tuples:\n",
    "            (\"img_array\", <frame_id>)               for resized colour images,\n",
    "            (\"T_matrix\",  <frame_id>)               for pose camera matrices\n",
    "\n",
    "        \n",
    "        <frame_id> is:\n",
    "            an integer (0 or 1) representing the temporal step relative to 'index',\n",
    "            0 --> t\n",
    "            1 --> t+1\n",
    "        \"\"\"\n",
    "        item={}\n",
    "\n",
    "        # Image ids\n",
    "        seq_id=int(self.seq_ids[index])\n",
    "        img_id=self.ind_map[index]\n",
    "\n",
    "        # If index= the last of the sequence, we need to work with the previous image\n",
    "        #max_img_id=int(max(list(self.total_dict[seq_id].keys())))\n",
    "        #if img_id==max_img_id:\n",
    "        #    step=-1\n",
    "        #else:\n",
    "        #    step=+1\n",
    "\n",
    "        step=1\n",
    "        \n",
    "        index_next=self.ind_map.index(img_id+step)\n",
    "\n",
    "        # Loading images + necessary transformation\n",
    "        img=np.array(Image.open(os.path.join(self.images_path, self.list_dir_images[index])))\n",
    "        img=cv2.resize(img, self.new_shape, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        img_next=np.array(Image.open(os.path.join(self.images_path, self.list_dir_images[index_next])))\n",
    "        img_next=img=cv2.resize(img_next, self.new_shape, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        if self.transform_images is not None:\n",
    "            img=self.transform_images(img)\n",
    "            img_next=self.transform_images(img_next)\n",
    "\n",
    "        # Later= data augmentation\n",
    "        # ...\n",
    "        #\n",
    "        \n",
    "        # Pose matrices\n",
    "        T=self.get_T_matrix(seq_id, img_id, self.total_dict)\n",
    "        T_next=self.get_T_matrix(seq_id, img_id+step, self.total_dict)\n",
    "\n",
    "        # Filling the dictionnary\n",
    "\n",
    "        item[(\"img_array\", 0)]=img\n",
    "        item[(\"img_array\", 1)]=img_next\n",
    "        item[(\"T_matrix\",  0)]=T\n",
    "        item[(\"T_matrix\",  1)]=T_next\n",
    "        \n",
    "        # Getting + resizing the image from the dataset folder\n",
    "        img = np.array(Image.open(os.path.join(self.images_path, self.list_dir_images[index])))\n",
    "        img=cv2.resize(img, self.new_shape, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        return item\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_dir_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the dataset\n",
    "\n",
    "batch_size=8\n",
    "train_paths=['/home/server/Ines/models/raw/train/images', '/home/server/Ines/models/raw/train/depth_maps']\n",
    "test_paths=['/home/server/Ines/models/raw/test/images', '/home/server/Ines/models/raw/test/depth_maps']\n",
    "network_name='self_supervised'\n",
    "new_shape=(256,128)\n",
    "\n",
    "train_dataset=Image_and_Pose_Pairs(train_paths, train_bin_paths, 'self_supervised')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06313325 -0.9976945  -0.02489732 -3.24288019]\n",
      " [-0.20279135  0.03725111 -0.97851317 -1.46815524]\n",
      " [ 0.97718466 -0.05672776 -0.2046756  -1.6055572 ]]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[243][(\"T_matrix\", 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    }
   ],
   "source": [
    "def create_list_tuples(dict, datasetname):\n",
    "    \"\"\"Input: dictionnary created thanks to colmap script + datasetname to correct the image name\n",
    "       Output\n",
    "\n",
    "    \"\"\"\n",
    "ms=[]\n",
    "for i in range(1, len(ms_objects)+1):\n",
    "    qvec, tvec, name= ms_objects[i].qvec, ms_objects[i].tvec, ms_objects[i].name\n",
    "    ms.append((qvec, tvec, name))\n",
    "print(len(ms))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_self_supervised(batch_size, train_im_paths, test_im_paths, train_bin_paths, test_bin_paths, new_shape, network_name):\n",
    "    \"\"\"\n",
    "    Input: Network parameters chosen by the user \n",
    "    Output : Pytorch train / test datasets and dataloaders \n",
    "    \"\"\"\n",
    "\n",
    "    ## Image transformation and useful parameters \n",
    "\n",
    "    transform_image=T.ToTensor()\n",
    "    test_batch_size=1\n",
    "\n",
    "    train_dataset=Image_and_Pose_Pairs(train_im_paths, train_bin_paths, network_name, transform_images=None, new_shape=(256,128))\n",
    "    test_dataset=Image_and_Pose_Pairs(test_im_paths, test_bin_paths, network_name, transform_images=None, new_shape=(256,128))\n",
    "\n",
    "    train_loader=DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_loader=DataLoader(test_dataset, test_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_dataset, test_dataset, train_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters \n",
    "\n",
    "#test_bin_paths=[ghost_city_bin_path, medium_structure_bin_path]\n",
    "#train_bin_paths=[old_cliff_bin_path, thermitiere_bin_path]\n",
    "\n",
    "batch_size=8\n",
    "train_im_paths=['/home/server/Ines/models/raw/train/images', '/home/server/Ines/models/raw/train/depth_maps']\n",
    "test_im_paths=['/home/server/Ines/models/raw/test/images', '/home/server/Ines/models/raw/test/depth_maps']\n",
    "network_name='self_supervised'\n",
    "new_shape=(256,128)\n",
    "\n",
    "## Test generation method\n",
    "\n",
    "train_dataset, test_dataset, train_loader, test_loader = generate_self_supervised(batch_size, train_im_paths, test_im_paths, train_bin_paths, test_bin_paths, new_shape, network_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX 0\n",
      "Pose matrix\n",
      "[[ 9.62124827e-01  2.71629800e-01 -2.30882819e-02 -2.58623899e+01]\n",
      " [ 1.84646765e-01 -7.11642995e-01 -6.77842031e-01  1.24432007e+01]\n",
      " [-2.00552710e-01  6.47905470e-01 -7.34844958e-01  4.53820553e+00]]\n",
      "INDEX 1\n",
      "Pose matrix\n",
      "[[ 9.61212758e-01  2.75100265e-01 -1.97453066e-02 -2.58964661e+01]\n",
      " [ 1.90533011e-01 -7.14081132e-01 -6.73635887e-01  1.26656769e+01]\n",
      " [-1.99417162e-01  6.43745276e-01 -7.38799577e-01  4.91454295e+00]]\n",
      "INDEX 2\n",
      "Pose matrix\n",
      "[[ 9.57116956e-01  2.88913716e-01 -2.13541009e-02 -2.59418909e+01]\n",
      " [ 1.99529952e-01 -7.10854545e-01 -6.74443188e-01  1.25056394e+01]\n",
      " [-2.10035547e-01  6.41260228e-01 -7.38017879e-01  5.35667896e+00]]\n",
      "INDEX 3\n",
      "Pose matrix\n",
      "[[  0.94843812   0.3088604    0.0712067  -26.59606946]\n",
      " [  0.29316248  -0.76939071  -0.56753299   9.91974163]\n",
      " [ -0.12050269   0.55914505  -0.82026576   3.17994192]]\n"
     ]
    }
   ],
   "source": [
    "ind=0\n",
    "for test_item in test_loader:\n",
    "    if ind<4:\n",
    "        print('INDEX', ind)\n",
    "        print('Pose matrix')\n",
    "        print(test_dataset[ind][(\"T_matrix\", 1)])\n",
    "        ind+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test computation reprojection loss:\n",
    "\n",
    "# Intrinsics matrix given in the trainer parameters \n",
    "\n",
    "K=None\n",
    "for train_item in train_loader:\n",
    "    ## Init\n",
    "    im=train_item(\"img_array\", 0)\n",
    "    next_im=train_item(\"img_array\", 1)\n",
    "\n",
    "    T=train_item(\"T_matrix\", 0)\n",
    "    T_next=train_item(\"T_matrix\", 1)\n",
    "\n",
    "    T_inv=np.pinv(T)\n",
    "    T_next_inv=np.pinv(T_next)\n",
    "\n",
    "    ## Proper device \n",
    "\n",
    "    ## Reprojection loss \n",
    "\n",
    "    # R t--> t+1\n",
    "\n",
    "    # R t+1 --> t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on the reprojection process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal= applying the reprojection process + loss computation on mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackprojectDepth(nn.Module):\n",
    "    \"\"\"Layer to transform a depth image into a point cloud\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, height, width):\n",
    "        super(BackprojectDepth, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        # Creating all the p=(u,v) pixels \n",
    "        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing='xy')\n",
    "        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n",
    "        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\n",
    "                                      requires_grad=False)\n",
    "\n",
    "        self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\n",
    "                                 requires_grad=False)\n",
    "\n",
    "        self.pix_coords = torch.unsqueeze(torch.stack(\n",
    "            [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\n",
    "        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\n",
    "        self.pix_coords =torch.cat([self.pix_coords, self.ones], 1)\n",
    "        #self.pix_coords =torch.transpose(self.pix_coords, 0 ,2)\n",
    "        #print(self.pix_coords.size())\n",
    "        #self.pix_coords =torch.transpose(self.pix_coords, 0 ,1)\n",
    "        self.pix_coords=nn.Parameter(self.pix_coords)\n",
    "\n",
    "                                       \n",
    "        # UNDERSTANDING PIX COORDS\n",
    "        \n",
    "        print('pix size', self.pix_coords.size())\n",
    "        \n",
    "\n",
    "    def forward(self, depth, inv_K):\n",
    "        print(self.pix_coords.size())\n",
    "        \n",
    "        cam_points = torch.matmul(inv_K[:, :3, :3], self.pix_coords)\n",
    "        cam_points = depth.view(self.batch_size, 1, -1) * cam_points\n",
    "        cam_points = torch.cat([cam_points, self.ones], 1)\n",
    "        print('cam points size', cam_points.size())\n",
    "\n",
    "        return cam_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pix size torch.Size([8, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "height=5\n",
    "width=2\n",
    "backproject_depth=BackprojectDepth(batch_size, height, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 10])\n",
      "cam points size torch.Size([8, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "K=torch.from_numpy(np.ones((8,3,4)))\n",
    "K=K.type(torch.FloatTensor)\n",
    "inv_K=np.linalg.pinv(K)\n",
    "inv_K=torch.from_numpy(inv_K)\n",
    "depth=torch.from_numpy(np.ones((8,height,width)))\n",
    "points_cam_ref=backproject_depth(depth, inv_K)\n",
    "# OK!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 10])\n",
      "torch.Size([8, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "## Understanding cam ref projection --> world ref projection.\n",
    "\n",
    "print(points_cam_ref.size())\n",
    "T=torch.from_numpy(np.ones((batch_size, 4,4)))\n",
    "points_world_ref=torch.matmul(T, points_cam_ref)\n",
    "print(points_world_ref.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From world ref to the pixel plane of the second camera\n",
    "\n",
    "class Project3D(nn.Module):\n",
    "    \"\"\"Layer which projects 3D points into a camera with intrinsics K and with inverse pose matrix T_inv\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, height, width, eps=1e-7):\n",
    "        super(Project3D, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, points_world_ref, K, T_inv):\n",
    "        K=K.type(torch.FloatTensor)\n",
    "        T_inv=T_inv.type(torch.FloatTensor)\n",
    "        points_world_ref=points_world_ref.type(torch.FloatTensor)\n",
    "        P = torch.matmul(K, T_inv)[:, :3, :]\n",
    "        print(P.size())\n",
    "        points_new_cam_ref=torch.matmul(P, points_world_ref)\n",
    "        pix_coords = points_new_cam_ref[:, :2, :] / (points_new_cam_ref[:, 2, :].unsqueeze(1) + self.eps)\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "        print( 'Pix size', pix_coords.size())\n",
    "      \n",
    "        \n",
    "        \n",
    "        pix_coords = pix_coords.view(self.batch_size, 2, self.height, self.width)\n",
    "        print( 'Pix size', pix_coords.size())\n",
    "        pix_coords = pix_coords.permute(0, 2, 3, 1)\n",
    "        print( 'Pix size', pix_coords.size())\n",
    "        pix_coords[..., 0] /= self.width - 1\n",
    "        print( 'Pix size', pix_coords.size())\n",
    "        pix_coords[..., 1] /= self.height - 1\n",
    "        print( 'Pix size', pix_coords.size())\n",
    "        #pix_coords = (pix_coords - 0.5) * 2\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 4])\n",
      "Pix size torch.Size([8, 2, 10])\n",
      "Pix size torch.Size([8, 2, 5, 2])\n",
      "Pix size torch.Size([8, 5, 2, 2])\n",
      "Pix size torch.Size([8, 5, 2, 2])\n",
      "Pix size torch.Size([8, 5, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "project3d=Project3D(batch_size, height, width)\n",
    "T_inv=torch.from_numpy(np.ones((batch_size, 4,4)))\n",
    "pix_coords=project3d(points_world_ref, K, T_inv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('monocular')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45f2bed9a0e297c8321bc81029f3c022b6b1f82f3e99e7b1fe109da730cb5db3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
